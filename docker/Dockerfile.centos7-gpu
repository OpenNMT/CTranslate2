FROM nvidia/cuda:10.0-cudnn7-devel-centos7 as builder

RUN yum install -y \
        boost-static \
        gcc \
        gcc-c++ \
        make \
        python \
        python-devel \
        wget && \
    rm -rf /var/cache/yum/*

WORKDIR /root

RUN wget https://cmake.org/files/v3.12/cmake-3.12.2-Linux-x86_64.tar.gz
RUN tar xf cmake-3.12.2-Linux-x86_64.tar.gz && \
    rm cmake-3.12.2-Linux-x86_64.tar.gz
ENV PATH=$PATH:/root/cmake-3.12.2-Linux-x86_64/bin

ENV MKL_VERSION=2019
ENV MKL_UPDATE=5
ENV MKL_BUILD=075
RUN yum-config-manager --add-repo https://yum.repos.intel.com/mkl/setup/intel-mkl.repo && \
    rpm --import https://yum.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS-2019.PUB && \
    yum install -y intel-mkl-64bit-$MKL_VERSION.$MKL_UPDATE-$MKL_BUILD && \
    rm -rf /var/cache/yum/*

ENV MKLDNN_ROOT=/root/mkl-dnn
ENV MKLDNN_VERSION=0.21
RUN wget https://github.com/intel/mkl-dnn/archive/v$MKLDNN_VERSION.tar.gz && \
    tar xf v$MKLDNN_VERSION.tar.gz && rm v$MKLDNN_VERSION.tar.gz && \
    cd mkl-dnn-* && \
    mkdir build && cd build && \
    cmake -DCMAKE_INSTALL_PREFIX=${MKLDNN_ROOT} -DARCH_OPT_FLAGS="" \
          -DMKLROOT=/opt/intel/mkl -DMKLDNN_USE_MKL=FULL:STATIC -DMKLDNN_THREADING=OMP:INTEL \
          -DWITH_TEST=OFF -DWITH_EXAMPLE=OFF .. && \
    make -j4 && make install && \
    cd ../.. && rm -r mkl-dnn-*

ENV TENSORRT_MAJOR_VERSION=5
ENV TENSORRT_VERSION=${TENSORRT_MAJOR_VERSION}.1.5
RUN curl -fsSL https://developer.download.nvidia.com/compute/machine-learning/repos/rhel7/x86_64/libnvinfer-devel-${TENSORRT_VERSION}-1.cuda10.0.x86_64.rpm -O && \
    curl -fsSL https://developer.download.nvidia.com/compute/machine-learning/repos/rhel7/x86_64/libnvinfer${TENSORRT_MAJOR_VERSION}-${TENSORRT_VERSION}-1.cuda10.0.x86_64.rpm -O && \
    rpm -ivh --nodeps libnvinfer*.rpm && \
    rm libnvinfer*.rpm

ENV CUB_VERSION=1.8.0
ENV CUB_ROOT=/root/cub
RUN wget https://github.com/NVlabs/cub/archive/v${CUB_VERSION}.tar.gz && \
    tar xf v${CUB_VERSION}.tar.gz && \
    mv cub-${CUB_VERSION} ${CUB_ROOT} && \
    rm v${CUB_VERSION}.tar.gz

WORKDIR /root/ctranslate2-dev

COPY cli cli
COPY include include
COPY src src
COPY tests tests
COPY CMakeLists.txt .

ARG CXX_FLAGS
ENV CXX_FLAGS=${CXX_FLAGS}
ARG CUDA_NVCC_FLAGS
ENV CUDA_NVCC_FLAGS=${CUDA_NVCC_FLAGS:-"-Xfatbin -compress-all"}
ARG CUDA_ARCH_LIST
ENV CUDA_ARCH_LIST=${CUDA_ARCH_LIST:-"Common"}

RUN mkdir build && \
    cd build && \
    cmake -DCMAKE_INSTALL_PREFIX=/root/ctranslate2 \
          -DCMAKE_PREFIX_PATH="${CUB_ROOT};${MKLDNN_ROOT}" \
          -DWITH_CUDA=ON -DWITH_MKLDNN=ON \
          -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="${CXX_FLAGS}" \
          -DCUDA_NVCC_FLAGS="${CUDA_NVCC_FLAGS}" -DCUDA_ARCH_LIST="${CUDA_ARCH_LIST}" .. && \
    VERBOSE=1 make -j4 && \
    make install

COPY python python

WORKDIR /root/ctranslate2-dev/python
RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python get-pip.py && rm get-pip.py
RUN pip --no-cache-dir install setuptools wheel
RUN CTRANSLATE2_ROOT=/root/ctranslate2 python setup.py bdist_wheel

WORKDIR /root
RUN cp /opt/intel/lib/intel64/libiomp5.so /root/ctranslate2/lib64 && \
    cp -P /root/mkl-dnn/lib64/libmkldnn.so* /root/ctranslate2/lib64 && \
    cp -P /usr/lib64/libboost_python*.so* /root/ctranslate2/lib64 && \
    cp -P /usr/local/cuda/lib64/libcudnn.so* /root/ctranslate2/lib64 && \
    cp -P /lib64/libnvinfer.so* /root/ctranslate2/lib64 && \
    cp /root/ctranslate2-dev/python/dist/*whl /root/ctranslate2

FROM nvidia/cuda:10.0-base-centos7

RUN yum install -y \
        cuda-cublas-$CUDA_PKG_VERSION && \
    rm -rf /var/cache/yum/*

COPY --from=builder /root/ctranslate2 /opt/ctranslate2
RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python get-pip.py && rm get-pip.py
RUN pip --no-cache-dir install /opt/ctranslate2/*.whl

WORKDIR /opt

ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/ctranslate2/lib64

ENTRYPOINT ["/opt/ctranslate2/bin/translate"]
