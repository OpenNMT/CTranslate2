<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quantization &mdash; CTranslate2 4.6.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.png"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Decoding features" href="decoding.html" />
    <link rel="prev" title="Model conversion" href="conversion.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> CTranslate2
          </a>
              <div class="version">
                4.6
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tasks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="translation.html">Text translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="generation.html">Text generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoding.html">Text encoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech_recognition.html">Speech recognition</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="conversion.html">Model conversion</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quantization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quantize-on-model-conversion">Quantize on model conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quantize-on-model-loading">Quantize on model loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="#implicit-type-conversion-on-load">Implicit type conversion on load</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-types">Supported types</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#bit-integers-int8">8-bit integers (<code class="docutils literal notranslate"><span class="pre">int8</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bit-integers-int16">16-bit integers (<code class="docutils literal notranslate"><span class="pre">int16</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bit-floating-points-float16">16-bit floating points (<code class="docutils literal notranslate"><span class="pre">float16</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bit-brain-floating-points-bfloat16">16-bit brain floating points (<code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bit-awq">4-bit AWQ</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="decoding.html">Decoding features</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel.html">Multithreading and parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory.html">Memory management</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment_variables.html">Environment variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Framework guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="guides/fairseq.html">Fairseq</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/marian.html">Marian</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/opennmt_py.html">OpenNMT-py</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/opennmt_tf.html">OpenNMT-tf</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/opus_mt.html">OPUS-MT</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/transformers.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="python/overview.html">Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="hardware_support.html">Hardware support</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently asked questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="versioning.html">Versioning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">CTranslate2</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Quantization</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Permalink to this heading"></a></h1>
<p>Quantization is a technique that can reduce the model size and accelerate its execution with little to no degradation in accuracy. CTranslate2 supports the most common types:</p>
<ul class="simple">
<li><p>8-bit integers (INT8)</p></li>
<li><p>16-bit integers (INT16)</p></li>
<li><p>16-bit floating points (FP16)</p></li>
<li><p>16-bit brain floating points (BF16)</p></li>
<li><p>4-bit AWQ Quantization</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>See the benchmark results in the main <a class="reference external" href="https://github.com/OpenNMT/CTranslate2#benchmarks">README</a> to compare the performance and memory usage with and without quantization.</p>
</div>
<section id="quantize-on-model-conversion">
<h2>Quantize on model conversion<a class="headerlink" href="#quantize-on-model-conversion" title="Permalink to this heading"></a></h2>
<p>Enabling the quantization when converting the model is helpful to reduce its size on disk. The converters expose the option <code class="docutils literal notranslate"><span class="pre">quantization</span></code> that accepts the following values:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">int8</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int8_float32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int8_float16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int8_bfloat16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">float16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bfloat16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">float32</span></code></p></li>
</ul>
<p>For example,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ct2-opennmt-py-converter<span class="w"> </span>--model_path<span class="w"> </span>model.pt<span class="w"> </span>--quantization<span class="w"> </span>int8<span class="w"> </span>--output_dir<span class="w"> </span>ct2_model
</pre></div>
</div>
<p>When the option <code class="docutils literal notranslate"><span class="pre">--quantization</span></code> is not set, the converted model will be saved with the same type as the original model (typically one of float32, float16, or bfloat16).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Whatever quantization type is selected here, the runtime ensures the model can be loaded and executed efficiently. This implies the model weights are possibly converted to another type when the model is loaded, see <a class="reference internal" href="#implicit-type-conversion-on-load"><span class="std std-ref">Implicit type conversion on load</span></a>.</p>
</div>
<p>For reference, the table below compares the model size on disk for a base Transformer model without shared embeddings and a vocabulary of size 32k:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Quantization</p></th>
<th class="head"><p>Model size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">float32</span></code></p></td>
<td><p>364MB</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">int16</span></code></p></td>
<td><p>187MB</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">float16</span></code></p></td>
<td><p>182MB</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">bfloat16</span></code></p></td>
<td><p>182MB</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">int8_float32</span></code></p></td>
<td><p>100MB</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">int8_float16</span></code></p></td>
<td><p>95MB</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">int8_bfloat16</span></code></p></td>
<td><p>95MB</p></td>
</tr>
</tbody>
</table>
</section>
<section id="quantize-on-model-loading">
<h2>Quantize on model loading<a class="headerlink" href="#quantize-on-model-loading" title="Permalink to this heading"></a></h2>
<p>Quantization can also be enabled or changed when loading the model. The translator exposes the option <code class="docutils literal notranslate"><span class="pre">compute_type</span></code> that accepts the following values:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">default</span></code>: keep the same quantization that was used during model conversion (see <a class="reference internal" href="#implicit-type-conversion-on-load"><span class="std std-ref">Implicit type conversion on load</span></a> for exceptions)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">auto</span></code>: use the fastest computation type that is supported on this system and device</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int8</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int8_float32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int8_float16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int8_bfloat16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">float16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">float32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bfloat16</span></code></p></li>
</ul>
<p>For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">translator</span> <span class="o">=</span> <span class="n">ctranslate2</span><span class="o">.</span><span class="n">Translator</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">compute_type</span><span class="o">=</span><span class="s2">&quot;int8&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Conversions between all types are supported. For example, you can convert a model with <code class="docutils literal notranslate"><span class="pre">quantization=&quot;int8&quot;</span></code> and then execute in full precision with <code class="docutils literal notranslate"><span class="pre">compute_type=&quot;float32&quot;</span></code>.</p>
</div>
</section>
<section id="implicit-type-conversion-on-load">
<h2>Implicit type conversion on load<a class="headerlink" href="#implicit-type-conversion-on-load" title="Permalink to this heading"></a></h2>
<p>By default, the runtime tries to use the type that is saved in the converted model as the computation type. However, if the current platform or backend do not support optimized execution for this computation type (e.g. <code class="docutils literal notranslate"><span class="pre">int16</span></code> is not optimized on GPU), then the library converts the model weights to another optimized type. The tables below document the fallback types in prebuilt binaries:</p>
<p><strong>On CPU:</strong></p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>int8_float32</p></th>
<th class="head"><p>int8_float16</p></th>
<th class="head"><p>int8_bfloat16</p></th>
<th class="head"><p>int16</p></th>
<th class="head"><p>float16</p></th>
<th class="head"><p>bfloat16</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>x86-64 (Intel)</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>int16</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
</tr>
<tr class="row-odd"><td><p>x86-64 (other)</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
</tr>
<tr class="row-even"><td><p>AArch64/ARM64 (Apple)</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
</tr>
<tr class="row-odd"><td><p>AArch64/ARM64 (other)</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
</tr>
</tbody>
</table>
<p><strong>On GPU:</strong></p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Compute Capability</p></th>
<th class="head"><p>int8_float32</p></th>
<th class="head"><p>int8_float16</p></th>
<th class="head"><p>int8_bfloat16</p></th>
<th class="head"><p>int16</p></th>
<th class="head"><p>float16</p></th>
<th class="head"><p>bfloat16</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>&gt;= 8.0</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float16</p></td>
<td><p>int8_bfloat16</p></td>
<td><p>float16</p></td>
<td><p>float16</p></td>
<td><p>bfloat16</p></td>
</tr>
<tr class="row-odd"><td><p>&gt;= 7.0, &lt; 8.0</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float16</p></td>
<td><p>int8_float32</p></td>
<td><p>float16</p></td>
<td><p>float16</p></td>
<td><p>float32</p></td>
</tr>
<tr class="row-even"><td><p>6.2</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
</tr>
<tr class="row-odd"><td><p>6.1</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>int8_float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
</tr>
<tr class="row-even"><td><p>&lt;= 6.0</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
<td><p>float32</p></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can get more information about the detected capabilities of your system by enabling the info logs (set the environment variable <code class="docutils literal notranslate"><span class="pre">CT2_VERBOSE=1</span></code> or call <code class="docutils literal notranslate"><span class="pre">ctranslate2.set_log_level(logging.INFO)</span></code>).</p>
<p>The supported compute types can also be queried at runtime with the Python function <a class="reference internal" href="python/ctranslate2.get_supported_compute_types.html"><span class="doc std std-doc"><code class="docutils literal notranslate"><span class="pre">ctranslate2.get_supported_compute_types</span></code></span></a>.</p>
</div>
</section>
<section id="supported-types">
<h2>Supported types<a class="headerlink" href="#supported-types" title="Permalink to this heading"></a></h2>
<section id="bit-integers-int8">
<h3>8-bit integers (<code class="docutils literal notranslate"><span class="pre">int8</span></code>)<a class="headerlink" href="#bit-integers-int8" title="Permalink to this heading"></a></h3>
<p><strong>Supported on:</strong></p>
<ul class="simple">
<li><p>NVIDIA GPU with Compute Capability &gt;= 7.0 or Compute Capability 6.1</p></li>
<li><p>x86-64 CPU with the Intel MKL or oneDNN backends</p></li>
<li><p>AArch64/ARM64 CPU with the Ruy backend</p></li>
</ul>
<p>The implementation applies the equation from <a class="reference external" href="https://arxiv.org/abs/1609.08144">Wu et al. 2016</a> to quantize the weights of the embedding and linear layers:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>scale[i] = 127 / max(abs(W[i,:]))

WQ[i,j] = round(scale[i] * W[i,j])
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This formula corresponds to a symmetric quantization (absolute maximum of the input range instead of separate min/max values).</p>
</div>
<p>Non quantized layers are run in the floating point precision of the original model. For example, if the model is saved in <code class="docutils literal notranslate"><span class="pre">float16</span></code>, the actual quantization type will be <code class="docutils literal notranslate"><span class="pre">int8_float16</span></code>. This behavior can be changed by selecting more specific quantization types:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">int8_float32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int8_float16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int8_bfloat16</span></code></p></li>
</ul>
</section>
<section id="bit-integers-int16">
<h3>16-bit integers (<code class="docutils literal notranslate"><span class="pre">int16</span></code>)<a class="headerlink" href="#bit-integers-int16" title="Permalink to this heading"></a></h3>
<p><strong>Supported on:</strong></p>
<ul class="simple">
<li><p>Intel CPU with the Intel MKL backend</p></li>
</ul>
<p>The implementation follows the work by <a class="reference external" href="https://arxiv.org/abs/1705.01991">Devlin 2017</a>. By default we use one quantization scale per layer. The scale is defined as:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>scale = 2^10 / max(abs(W))
</pre></div>
</div>
<p>As suggested by the author, the idea is to use 10 bits for the input so that the multiplication is 20 bits which gives 12 bits left for accumulation.</p>
<p>Similar to the <code class="docutils literal notranslate"><span class="pre">int8</span></code> quantization, only the weights of the embedding and linear layers are quantized to 16-bit integers. The other layers are run in FP32.</p>
</section>
<section id="bit-floating-points-float16">
<h3>16-bit floating points (<code class="docutils literal notranslate"><span class="pre">float16</span></code>)<a class="headerlink" href="#bit-floating-points-float16" title="Permalink to this heading"></a></h3>
<p><strong>Supported on:</strong></p>
<ul class="simple">
<li><p>NVIDIA GPU with Compute Capability &gt;= 7.0</p></li>
</ul>
<p>In this mode, all model weights are stored in half precision and all layers are run in half precision.</p>
</section>
<section id="bit-brain-floating-points-bfloat16">
<h3>16-bit brain floating points (<code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>)<a class="headerlink" href="#bit-brain-floating-points-bfloat16" title="Permalink to this heading"></a></h3>
<p><strong>Supported on:</strong></p>
<ul class="simple">
<li><p>NVIDIA GPU with Compute Capability &gt;= 8.0</p></li>
</ul>
<p>In this mode, all model weights are stored in BF16 and all layers are run with this type.</p>
</section>
<section id="bit-awq">
<h3>4-bit AWQ<a class="headerlink" href="#bit-awq" title="Permalink to this heading"></a></h3>
<p><strong>Supported on:</strong></p>
<ul class="simple">
<li><p>NVIDIA GPU with Compute Capability &gt;= 7.5</p></li>
</ul>
<p>CTranslate2 internally handles the compute type for AWQ quantization.
In this mode, all model weights are stored in half precision and all layers are run in half precision. Other parameters like scale and zero are stored in <code class="docutils literal notranslate"><span class="pre">int32</span></code>.</p>
<p><strong>Steps to use AWQ Quantization:</strong></p>
<ul class="simple">
<li><p>Download a AWQ quantized model from Hugging Face for example (TheBloke/Llama-2-7B-AWQ){https://huggingface.co/TheBloke/Llama-2-7B-AWQ} or quantize your own model with using this (AutoAWQ example){https://casper-hansen.github.io/AutoAWQ/examples/}.</p></li>
<li><p>Convert AWQ Quantized model to Ctranslate2 model:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>ct2-transformers-converter<span class="w"> </span>--model<span class="w"> </span>TheBloke/Llama-2-7B-AWQ<span class="w"> </span>--copy_files<span class="w"> </span>tokenizer.model<span class="w"> </span>--output_dir<span class="w"> </span>ct2_model
</pre></div>
</div>
<ul class="simple">
<li><p>Run inference as usual with Ctranslate2:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>ctranslate2.Generator<span class="o">(</span><span class="s1">&#39;ct2_model&#39;</span>,<span class="w"> </span><span class="nv">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="o">)</span>
<span class="nv">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>model.generate_batch<span class="o">([</span>tokens<span class="o">])</span>
</pre></div>
</div>
<p>Currently, CTranslate2 only supports the GEMM and GEMV kernels for AWQ quantization.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="conversion.html" class="btn btn-neutral float-left" title="Model conversion" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="decoding.html" class="btn btn-neutral float-right" title="Decoding features" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>