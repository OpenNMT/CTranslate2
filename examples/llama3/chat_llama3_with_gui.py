import os
import sys
import ctranslate2
from transformers import AutoTokenizer
from PySide6.QtWidgets import QApplication, QMainWindow, QVBoxLayout, QHBoxLayout, QTextEdit, QLineEdit, QPushButton, QWidget
from PySide6.QtGui import QTextCursor, QCloseEvent
from PySide6.QtCore import Signal, QObject, QThread

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

MODEL_LOCATION = [specify local path or huggingface repo id] # local path to ct2 model or huggingface repo id
SYSTEM_PROMPT = "Your are a helpful and courteous assistant who tries to help the person you're speaking with." # change to your liking
DEVICE = "cuda" # set to cpu if you want
COMPUTE_TYPE = "int8" # set to float16 etc.
INTRA_THREADS = 4 # set number of cpu threads; ok to keep this parameter even when using CUDA
FLASH_ATTENTION = True # Can only use "True" on Ampere or later GPUs


class ChatLlama3:
    """
    A class to manage the ChatLlama3 model's translation and tokenization using the ctranslate2 and transformers libraries.
    
    Attributes:
        generator (ctranslate2.Generator): The model generator, set up to use specified computation settings and device.
        tokenizer (transformers.AutoTokenizer): The tokenizer for processing and managing input tokens.
        context_length (int): The total number of tokens the model can handle in a single prompt.
        max_generation_length (int): The maximum number of tokens that can be generated in a response.
        max_prompt_length (int): Calculated maximum length for a prompt, to ensure response size is within the limit.
        eos_token_id (int): End-of-sentence token ID for marking the end of a sentence.
        eot_token_id (int): End-of-text token ID used for marking the end of the text.
        end_tokens (list): List containing end tokens for generation processes.
        messages (list): List of dictionaries that store the role and content of each part of the conversation.
    
    Methods:
        generate_response(user_prompt, response_callback=None): Process the user's prompt and generate a response.
        generate_words(step_results): Yields words decoded from token IDs generated by the model.
    """
    def __init__(self):
        print("Loading the model...")
        self.generator = ctranslate2.Generator(MODEL_LOCATION, device=DEVICE, compute_type=COMPUTE_TYPE, intra_threads=INTRA_THREADS, flash_attention=FLASH_ATTENTION)
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCATION)
        self.context_length = 4096
        self.max_generation_length = 512
        self.max_prompt_length = self.context_length - self.max_generation_length
        self.eos_token_id = self.tokenizer.eos_token_id
        self.eot_token_id = self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        self.end_tokens = [self.eos_token_id, self.eot_token_id]
        self.messages = []

        if SYSTEM_PROMPT:
            self.messages.append({"role": "system", "content": SYSTEM_PROMPT})

    def generate_response(self, user_prompt, response_callback=None):
        self.messages.append({"role": "user", "content": user_prompt})

        while True:
            input_ids = self.tokenizer.apply_chat_template(
                self.messages,
                add_generation_prompt=True,
                return_tensors="np"
            )

            if len(input_ids[0]) <= self.max_prompt_length:
                break

            # Remove old conversations when prompt size becomes too large
            if SYSTEM_PROMPT:
                self.messages = [self.messages[0]] + self.messages[3:]
            else:
                self.messages = self.messages[2:]

        prompt_tokens = [self.tokenizer.convert_ids_to_tokens(ids) for ids in input_ids.tolist()]

        step_results = self.generator.generate_tokens(
            prompt_tokens,
            max_length=self.max_generation_length,
            sampling_temperature=0.6,
            sampling_topk=20,
            sampling_topp=1,
            end_token=self.end_tokens,
        )

        text_output = ""
        for word in self.generate_words(step_results):
            text_output += word
            if response_callback:
                response_callback(word)
            else:
                print(word, end="", flush=True)

        self.messages.append({"role": "assistant", "content": text_output.strip()})
        return text_output.strip()

    def generate_words(self, step_results):
        tokens_buffer = []

        for step_result in step_results:
            is_new_word = step_result.token.startswith("Ä ")
            if is_new_word and tokens_buffer:
                word = self.tokenizer.decode(tokens_buffer)
                if word:
                    yield word
                tokens_buffer = []
            tokens_buffer.append(step_result.token_id)

        if tokens_buffer:
            word = self.tokenizer.decode(tokens_buffer)
            if word:
                yield word

class ChatWorker(QObject):
    """
    A worker class designed to handle response generation in a separate thread using Qt's signal and slot mechanism.
    
    Signals:
        response_generated (Signal): Emitted with each generated piece of text or word as the argument.
    
    Attributes:
        chat_llama3 (ChatLlama3): Instance of ChatLlama3 used for generating responses to user prompts.
    
    Methods:
        generate_response(user_prompt): Takes user prompt and triggers the response generation process in ChatLlama3.
        response_callback(word): Emits the 'response_generated' signal with the generated word.
    """
    response_generated = Signal(str)

    def __init__(self, chat_llama3):
        super().__init__()
        self.chat_llama3 = chat_llama3

    def generate_response(self, user_prompt):
        self.chat_llama3.generate_response(user_prompt, self.response_callback)

    def response_callback(self, word):
        self.response_generated.emit(word)

class ChatLlama3GUI(QMainWindow):
    """
    Main GUI class for the ChatLlama3 application, built using PySide6.
    
    Attributes:
        chat_llama3 (ChatLlama3): Instance of the ChatLlama3 class to manage response generation.
        chat_worker (ChatWorker): Worker object to manage responses asynchronously.
        thread (QThread): Thread to run chat_worker operations asynchronously.
        chat_history (QTextEdit): Widget to display the conversation history.
        user_input (QLineEdit): Input widget for the user to type their messages.
    
    Methods:
        send_message(): Handles sending messages from the user input to the chat worker and updates the GUI accordingly.
        update_chat_history(word): Updates the chat history in the GUI as new words are generated.
        closeEvent(event): Handles the closing event of the window, ensuring threads are properly closed.
    """
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Chat with Llama3")
        self.setGeometry(100, 100, 600, 800)

        self.chat_llama3 = ChatLlama3()
        self.chat_worker = ChatWorker(self.chat_llama3)
        self.thread = QThread()
        self.chat_worker.moveToThread(self.thread)
        self.thread.start()

        central_widget = QWidget(self)
        self.setCentralWidget(central_widget)

        layout = QVBoxLayout(central_widget)

        self.chat_history = QTextEdit(self)
        self.chat_history.setReadOnly(True)
        layout.addWidget(self.chat_history)

        input_layout = QHBoxLayout()

        self.user_input = QLineEdit(self)
        self.user_input.setPlaceholderText("Type your message...")
        self.user_input.returnPressed.connect(self.send_message)
        input_layout.addWidget(self.user_input)

        send_button = QPushButton("Send", self)
        send_button.clicked.connect(self.send_message)
        input_layout.addWidget(send_button)

        layout.addLayout(input_layout)

        self.chat_worker.response_generated.connect(self.update_chat_history)

    def send_message(self):
        user_prompt = self.user_input.text()
        self.user_input.clear()
        self.chat_history.append(f"You: {user_prompt}")
        self.chat_history.append("Llama3: ")
        self.chat_worker.generate_response(user_prompt)

    def update_chat_history(self, word):
        cursor = self.chat_history.textCursor()
        cursor.movePosition(QTextCursor.End)
        self.chat_history.setTextCursor(cursor)
        self.chat_history.insertPlainText(word)
        QApplication.processEvents()

    def closeEvent(self, event: QCloseEvent):
        self.thread.quit()
        self.thread.wait()
        event.accept()

if __name__ == "__main__":
    app = QApplication(sys.argv)
    app.setStyle("Fusion")
    chat_gui = ChatLlama3GUI()
    chat_gui.show()
    sys.exit(app.exec())