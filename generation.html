<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Text generation &mdash; CTranslate2 4.2.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.png"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Text encoding" href="encoding.html" />
    <link rel="prev" title="Text translation" href="translation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> CTranslate2
          </a>
              <div class="version">
                4.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tasks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="translation.html">Text translation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Text generation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#token-streaming">Token streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prompt-caching">Prompt caching</a></li>
<li class="toctree-l2"><a class="reference internal" href="#special-tokens">Special tokens</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="encoding.html">Text encoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech_recognition.html">Speech recognition</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="conversion.html">Model conversion</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="decoding.html">Decoding features</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel.html">Multithreading and parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory.html">Memory management</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment_variables.html">Environment variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Framework guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="guides/fairseq.html">Fairseq</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/marian.html">Marian</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/opennmt_py.html">OpenNMT-py</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/opennmt_tf.html">OpenNMT-tf</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/opus_mt.html">OPUS-MT</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides/transformers.html">Transformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="python/overview.html">Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="hardware_support.html">Hardware support</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently asked questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="versioning.html">Versioning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">CTranslate2</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Text generation</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="text-generation">
<h1>Text generation<a class="headerlink" href="#text-generation" title="Permalink to this headline"></a></h1>
<p>CTranslate2 exposes high-level classes to run generative language models such as <a class="reference external" href="https://github.com/openai/gpt-2">GPT-2</a>. The main entrypoint is the <a class="reference internal" href="python/ctranslate2.Generator.html"><span class="doc std std-doc"><code class="docutils literal notranslate"><span class="pre">Generator</span></code></span></a> class which provides several methods:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">generate_batch</span></code></p></td>
<td><p>Generate text from a batch of prompts or start tokens.</p></td>
<td><p><a class="reference internal" href="guides/transformers.html#gpt-2"><span class="std std-ref">GPT-2</span></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">score_batch</span></code></p></td>
<td><p>Compute the token-level log-likelihood and the sequence perplexity.</p></td>
<td><p><a class="reference internal" href="guides/fairseq.html#wmt19-language-model"><span class="std std-ref">WMT19 language model</span></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">generate_tokens</span></code></p></td>
<td><p>Stream the generated tokens.</p></td>
<td><p><a class="reference internal" href="#token-streaming"><span class="std std-ref">Token streaming</span></a><br/><a class="reference external" href="https://github.com/OpenNMT/CTranslate2/tree/master/examples/llama2">Chat with Llama 2</a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">forward_batch</span></code></p></td>
<td><p>Get the full output logits (or log probs) for a sequence.</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<section id="token-streaming">
<h2>Token streaming<a class="headerlink" href="#token-streaming" title="Permalink to this headline"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">generate_tokens</span></code> is a convenience method to return tokens as they are generated by the model. This can be useful when running large models in an interactive environment.</p>
<p>The example below shows how to use this method and progressively decode SentencePiece tokens. It should be adapted if the model uses a different tokenizer or the generated language does not use a space to separate words.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ctranslate2</span>
<span class="kn">import</span> <span class="nn">sentencepiece</span> <span class="k">as</span> <span class="nn">spm</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">ctranslate2</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="s2">&quot;ct2_model/&quot;</span><span class="p">)</span>
<span class="n">sp</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">(</span><span class="s2">&quot;tokenizer.model&quot;</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;What is the meaning of life?&quot;</span>
<span class="n">prompt_tokens</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>

<span class="n">step_results</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">generate_tokens</span><span class="p">(</span>
    <span class="n">prompt_tokens</span><span class="p">,</span>
    <span class="n">sampling_temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">sampling_topk</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">output_ids</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">step_result</span> <span class="ow">in</span> <span class="n">step_results</span><span class="p">:</span>
    <span class="n">is_new_word</span> <span class="o">=</span> <span class="n">step_result</span><span class="o">.</span><span class="n">token</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;▁&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_new_word</span> <span class="ow">and</span> <span class="n">output_ids</span><span class="p">:</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_ids</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">output_ids</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">output_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step_result</span><span class="o">.</span><span class="n">token_id</span><span class="p">)</span>

<span class="k">if</span> <span class="n">output_ids</span><span class="p">:</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_ids</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you <code class="docutils literal notranslate"><span class="pre">break</span></code> out of the loop, the generation will still run to completion in the background. To stop the generation early you should close the generator, for example using <code class="docutils literal notranslate"><span class="pre">step_results.close()</span></code>.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>The <code class="docutils literal notranslate"><span class="pre">callback</span></code> argument in the method <code class="docutils literal notranslate"><span class="pre">generate_batch</span></code> which can also be used to implement token streaming. This is what <code class="docutils literal notranslate"><span class="pre">generate_tokens</span></code> use internally.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>The example <a class="reference external" href="https://github.com/OpenNMT/CTranslate2/tree/master/examples/llama2">Chat with Llama 2</a> which uses token streaming in an interactive chat session.</p>
</div>
</section>
<section id="prompt-caching">
<h2>Prompt caching<a class="headerlink" href="#prompt-caching" title="Permalink to this headline"></a></h2>
<p>The methods <code class="docutils literal notranslate"><span class="pre">generate_batch</span></code> and <code class="docutils literal notranslate"><span class="pre">generate_tokens</span></code> have an argument <code class="docutils literal notranslate"><span class="pre">static_prompt</span></code> that can be used for models that always start with the same prompt (also known as a system prompt). The model is run once on this static prompt and the model state is cached and reused for future calls with the same static prompt.</p>
<p>For example <a class="reference external" href="https://github.com/Stability-AI/StableLM">StableLM</a> uses a system prompt which could be implemented like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ctranslate2</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">ctranslate2</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="s2">&quot;stablelm-ct2/&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;stabilityai/stablelm-tuned-alpha-7b&quot;</span><span class="p">)</span>

<span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;|SYSTEM|&gt;# StableLM Tuned (Alpha version)</span>
<span class="s2">- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.</span>
<span class="s2">- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.</span>
<span class="s2">- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.</span>
<span class="s2">- StableLM will refuse to participate in anything that could harm a human.</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">system_prompt_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">system_prompt</span><span class="p">))</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&lt;|USER|&gt;What&#39;s your mood today?&lt;|ASSISTANT|&gt;&quot;</span>
<span class="n">prompt_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span>

<span class="n">step_results</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">generate_tokens</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt_tokens</span><span class="p">,</span>
    <span class="n">static_prompt</span><span class="o">=</span><span class="n">system_prompt_tokens</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">sampling_topk</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">sampling_temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">end_token</span><span class="o">=</span><span class="p">[</span><span class="mi">50278</span><span class="p">,</span> <span class="mi">50279</span><span class="p">,</span> <span class="mi">50277</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At this time the cache size is unlimited and the cache is only cleared when the model is unloaded. Also if the model is loaded on multiple GPUs, each model replica manages its own cache to avoid copying the state between devices.</p>
</div>
</section>
<section id="special-tokens">
<h2>Special tokens<a class="headerlink" href="#special-tokens" title="Permalink to this headline"></a></h2>
<p>Special tokens such as the decoder start token <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code> should be explicitly included in the input if required by the model. No special tokens are added by the generator methods.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is different from the translator methods which usually include these special tokens implicitly.</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="translation.html" class="btn btn-neutral float-left" title="Text translation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="encoding.html" class="btn btn-neutral float-right" title="Text encoding" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>